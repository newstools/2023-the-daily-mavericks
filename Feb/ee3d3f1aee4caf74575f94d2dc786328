When people think about artificial intelligence (AI), they may have visions of the future. But AI is already here. At its base, it is the recreation of aspects of human intelligence in computerised form. Like human intelligence, it has wide application. Voice-operated personal assistants like Siri, self-driving cars, and text and image generators all use AI. It also curates our social media feeds. It helps companies to detect fraud and hire employees. It’s used to manage livestock, enhance crop yields and aid medical diagnoses. Alongside its growing power and its potential, AI raises moral and ethical questions. The technology has already been at the centre of multiple scandals: the infringement of laws and rights, as well as racial and gender discrimination. In short, it comes with a litany of ethical risks and dilemmas. But what exactly are these risks? And how do they differ among countries? To find out, I undertook a thematic review of literature from wealthier countries to identify six high-level, universal ethical risk themes. I then interviewed experts involved in or associated with the AI industry in South Africa and assessed how their perceptions of AI risk differed from or resonated with those themes. The findings reflect marked similarities in AI risks between the global north and South Africa as an example of a global south nation. But there were some important differences. These reflect South Africa’s unequal society and the fact that it is on the periphery of AI development, utilisation and regulation. Other developing countries that share similar features – a vast digital divide, high inequality and unemployment and low-quality education – likely have a similar risk profile to South Africa. Knowing what ethical risks may play out at a country level is important because it can help policymakers and organisations to adjust their risk management policies and practices accordingly. The six universal ethical risk themes I drew from reviewing global north literature were: Then I interviewed 16 experts involved in or associated with South Africa’s AI industry. They included academics, researchers, designers of AI-related products, and people who straddled the categories. For the most part, the six themes I’d already identified resonated with them. But the participants also identified five ethical risks that reflected South Africa’s country-level features. These were: So, what do these findings tell us? Firstly, the universal risks are mostly technical. They are linked to the features of AI and have technical solutions. For instance, bias can be mitigated by more accurate models and comprehensive data sets. Most of the South African-specific risks are more socio-technical, manifesting the country’s environment. An absence of policy and regulation, for example, is not an inherent feature of AI. It is a symptom of the country being on the periphery of technology development and related policy formulation. South African organisations and policymakers should therefore not just focus on technical solutions but also closely consider AI’s socio-economic dimensions. Secondly, the low levels of awareness among the population suggest there is little pressure on South African organisations to demonstrate a commitment to ethical AI. In contrast, organisations in the global north have to show cognisance of AI ethics, because their stakeholders are more attuned to their rights vis-à-vis digital products and services. Finally, whereas the EU, UK and US have nascent rules and regulations around AI, South Africa has no regulations and limited laws relevant to AI. The South African government has also failed to give much recognition to AI’s broader impact and ethical implications. This differs even from other emerging markets such as Brazil, Egypt, India, and Mauritius, which have national policies and strategies that encourage the responsible use of AI. AI may, for now, seem far removed from South Africa’s prevailing socio-economic challenges. But it will become pervasive in the coming years. South African organisations and policymakers should proactively govern AI ethics risks. This starts with acknowledging that AI presents threats that are distinct from those in the global north, and that need to be managed. Governing boards should add AI ethics to their agendas, and policymakers and members of governing boards should become educated on the technology. Additionally, AI ethics risks should be added to corporate and government risk management strategies – similar to climate change, which received scant attention 15 or 20 years ago but now features prominently. Perhaps most importantly, the government should build on the recent launch of the Artificial Intelligence Institute of South Africa, and introduce a tailored national strategy and appropriate regulation to ensure the ethical use of AI. DM/ML This story was first published in The Conversation. Emile Ormond is a PhD candidate at the University of South Africa.