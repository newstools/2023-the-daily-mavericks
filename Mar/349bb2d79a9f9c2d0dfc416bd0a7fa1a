TheÂ generative AIÂ industry will be worth aboutÂ A$22-trillion by 2030, according to the Commonwealth Scientific and Industrial Research Organisation (Cairo). These systems â€“ of which ChatGPT is currently the best known â€“Â canÂ write essays and code, generate music and artwork, and have entire conversations. But what happens when theyâ€™re turned to illegal uses? Last week, the streaming community was rockedÂ by a headline that links back to the misuse of generative AI. Popular Twitch streamer Atrioc issued an apology video, teary-eyed, after being caught viewing pornography with the superimposed faces of other women streamers. The â€œdeepfakeâ€ technology needed to Photoshop a celebrityâ€™s head on aÂ porn actorâ€™s body has been around for a while, but recent advances have made it much harder to detect. And thatâ€™s the tip of the iceberg. In the wrong hands, generative AI could do untold damage. Thereâ€™s a lot we stand to lose, should laws and regulations fail to keep up.  Last month, generative AI appÂ Lensa came under fireÂ for allowing its system to create fully nude and hypersexualised images from usersâ€™ headshots. Controversially, it also whitened the skin of women of colour and made their featuresÂ more European. The backlash was swift. But whatâ€™s relatively overlooked is the vast potential to use artistic generative AI in scams. At the far end of the spectrum, there are reports of these tools being able toÂ fake fingerprints and facial scansÂ (the method most of us use to lock our phones). Criminals are quickly finding new ways to use generative AI to improve the frauds they already perpetrate. The lure of generative AI in scams comes from its ability to find patterns inÂ large amounts of data. Cybersecurity has seen a rise in â€œbad botsâ€: malicious automated programs that mimic human behaviourÂ to conduct crime. Generative AI will make these even more sophisticated and difficult to detect. Ever receivedÂ a scam textÂ from the â€œtax officeâ€ claiming you had aÂ refund waiting? Or maybe you got a call claiming a warrant wasÂ out for your arrest? In such scams, generative AI could be used to improve the quality of texts or emails, making them much more believable. For example, in recent years weâ€™ve seen AI systems beingÂ used toÂ impersonate important figures in â€œvoice spoofingâ€ attacks. Then there areÂ romance scams, where criminals pose as romantic interests and ask their targets for money to help them out of financial distress. These scams are already widespread and often lucrative. Training AI on actual messages between intimate partners could help create a scam chatbot thatâ€™s indistinguishableÂ from a human. Generative AI could also allow cybercriminals to more selectively target vulnerable people. For instance, training a system on information stolen from major companies, such as in the Optus or Medibank hacks last year,Â could help criminals target elderly people, people with disabilities, or people in financial hardship. Further, these systems can be used toÂ improve computer code, which some cybersecurity experts say will make malware and viruses easier to create and harderÂ to detect for antivirus software. The US has had a legislatedÂ National Artificial Intelligence InitiativeÂ in place since 2021. And since 2019 it has beenÂ illegal in California for a bot to interact with users for commerce or electoral purposes without disclosing itâ€™s not human. The European Union is also well on the way to enacting the worldâ€™sÂ first AI law. The AI Act bans certain types of AI programs posing â€œunacceptable riskâ€ â€“ such as those used by Chinaâ€™sÂ social credit system â€“ and imposes mandatory restrictions on â€œhigh-riskâ€ systems. this seems exactly right to me. ğŸ¯ pic.twitter.com/hNXEDdN0Yn â€” Kareem Carr | Data Scientist (@kareem_carr) January 29, 2023  Although asking ChatGPTÂ to break the lawÂ results in warnings that â€œplanning or carrying out a serious crime can lead to severe legal consequencesâ€, the fact is thereâ€™s no requirement for these systems to have a â€œmoral codeâ€Â programmed into them. There may be no limit to what they can be asked to do, and criminals will likely figure out workarounds for any rules intended to prevent their illegal use. Governments need to work closely with the cybersecurity industry to regulate generative AI without stifling innovation, such as byÂ requiring ethical considerations for AI programs. (â€¦) As criminals add generative AI tools to their arsenal, spotting scams will only get trickier. TheÂ classic tips will still apply â€“ but beyond those, weâ€™ll learn a lot from assessing the ways in which these tools fall short. Generative AI is bad atÂ critical reasoning and conveying emotion. It can even be tricked intoÂ giving wrong answers. Knowing when and why this happens could us help develop effective methods to catch cybercriminals using AI for extortion. There are also tools being developed toÂ detect AI outputs from tools such as ChatGPT. These could go a long way towards preventing AI-based cybercrime if they prove to be effective.Â DM/ML This story was first published inÂ The Conversation. Brendan Walker-Munro is a Senior Research Fellow at The University of Queensland.