If you haven’t had your head under a rock over the holiday season, you will know that an artificial intelligence (AI) software application called ChatGPT came tearing out of the gates of a company called OpenAI in December, a little over a month ago. There was almost no pre-launch advertising, but it took just one week to attract one million users, faster than any software launch in history, even those with $100-million advertising campaigns. Within its short life, ChatGPT has generated a library of heated controversy, a cesspool of Twitter insults and counter-insults, dire predictions about the end of humanity as we know it and breathless prose about a brave new world. What in the world is this thing, and what does it portend? So, a little history. The field of AI has burned quietly for decades, all the way back to the 1950s. It has, on numerous occasions, raised expectations and then dashed them. I studied AI at university decades ago when hopes were high, but nothing really exciting happened until the last 10 years, when a confluence of new maths, cheap mass data storage and accessible and blazing CPU power saw the arrival of truly useful AI doing smart things that were tiresome, cumbersome or just plain difficult for humans to do. The real kicker was the new field of machine learning (ML), which was once a subset of AI, but now that relationship seems to have flipped. The concept is simple. Traditional computing takes some input, feeds it to a computer program that has been conceived and divined and coded by one or more humans to do a task, and produces an output. If there are no bugs and everything works as planned, you get Facebook or Adobe Acrobat or TikTok or an accounting program or a videogame. Machine learning is different. You also feed it input, but these are labelled examples of something, like cat images. The labels say “cat” or “not cat”. The machine-learning algorithm “learns” by example as to what comprises a “cat” image, using fancy maths and stats. That’s it. No program needs to be written. Just the training data set is required. If, after training,  you feed it an elephant image and it says: “This is a cat”, then you need more training data (or a more sophisticated machine-learning package). If you feed it a photo of a cat standing on its head and smoking a cigarette, and it says: “This is a cat”, then your application is well-trained. And it is a shortish step from here to facial recognition software, AI-assisted radiology, self-driving cars, language translation programs, etc. (Software nerds reading this will notice that I have ignored an entire field of machine learning called unsupervised learning, where the training data are not labelled. But we digress…) Okay — on to ChatGPT. Visit Daily Maverick’s home page for more news, analysis and investigations If you sign up to ChatGPT, you will be presented with an empty dialogue box. Just ask it any question. Really. Any question. Long question, short question, qualified question. About anything. In any sort of English. Never mind vernacular, typos and spelling errors. ChatGPT will understand. For instance: You get the point. It will answer (almost) any question at all, at multi-paragraph length if necessary, and in perfect English and with competent structure. If you don’t like the way the response was phrased, ask for another — thereby outsmarting most plagiarism-catching software. Many educators think it is a nightmare. Others take a softer view. Maybe the child didn’t write the essay, but they probably read it, and that is not a bad thing. And software development education faces a challenge. Practice makes perfect when learning programming, but if ChatGPT can write a program in five seconds that would have taken a student two weeks, then there be demons. But for hundreds of thousands of others, it is immediately and enormously helpful. I recently had a commercial writing assignment for an overseas client. I estimate that ChatGPT cut my research time by about 90% — it was far more efficient than slogging through Google. I could have asked ChatGPT to write the entire article, it was only guilt that stopped me; it seemed somehow dishonest, so I wrote it myself. But I felt the temptation. There is, not surprisingly, a whole online community dedicated to proving how stupid ChatGPT is and that reprints dumb or whacko answers to tough questions. These people consider ChatGPT to be a smart-aleck parlour trick. But they miss the point. Which is that ChatGPT is literally just out of the womb, it has been alive for almost no time and it is constantly learning and refining. Consider this. ChatGPT is built on a language model called GPT-3 that has 175 billion parameters. The definition of a parameter is less important than this: the next version of GPT-3 (GPT-4, to be released in a year or so) will have one hundred trillion parameters.  That is the same number of synapses we have in our brains. Does this mean that some future AI will one day be more intelligent, more creative and ultimately more evolutionarily fit for purpose than humans? These arguments rage back and forth among those who spend their lives researching these matters; it is great fun to watch how exercised people get about their biases. But a few things can be said with certainty. One is that AI (particularly in the form of ChatGPT) is now of immediate and startling benefit to even the most technically naïve user, without regard to age or speciality or background. And more importantly, we ain’t seen nothing yet. GPT-4 will change our lives in ways we cannot even imagine today. DM Steven Boykey Sidley is a professor of practice at JBS, University of Johannesburg. He is the author of seven books.